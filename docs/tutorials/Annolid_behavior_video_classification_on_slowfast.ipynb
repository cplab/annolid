{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8a2187f5",
      "metadata": {
        "id": "8a2187f5"
      },
      "source": [
        "# SlowFast\n",
        "\n",
        "*Author: FAIR PyTorchVideo*\n",
        "\n",
        "Modified from: https://pytorch.org/hub/facebookresearch_pytorchvideo_slowfast/\n",
        "\n",
        "**SlowFast networks pretrained fine-tuning on the custom behavior dataset**\n",
        "\n",
        "#### Install required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fvcore"
      ],
      "metadata": {
        "id": "_nnS3TXkm0uH"
      },
      "id": "_nnS3TXkm0uH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyav"
      ],
      "metadata": {
        "id": "qev6RUwGnAiZ"
      },
      "id": "qev6RUwGnAiZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84597b6a",
      "metadata": {
        "id": "84597b6a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.hub import load\n",
        "\n",
        "# Load the pre-trained SlowFast model\n",
        "model = load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37a61a8c",
      "metadata": {
        "id": "37a61a8c"
      },
      "source": [
        "Import remaining functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e79d46ea",
      "metadata": {
        "id": "e79d46ea"
      },
      "outputs": [],
      "source": [
        "from typing import Dict\n",
        "import json\n",
        "import urllib\n",
        "from torchvision.transforms import Compose, Lambda\n",
        "from torchvision.transforms._transforms_video import (\n",
        "    CenterCropVideo,\n",
        "    NormalizeVideo,\n",
        ")\n",
        "from pytorchvideo.data.encoded_video import EncodedVideo\n",
        "from pytorchvideo.transforms import (\n",
        "    ApplyTransformToKey,\n",
        "    ShortSideScale,\n",
        "    UniformTemporalSubsample,\n",
        "    UniformCropVideo\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef056027",
      "metadata": {
        "id": "ef056027"
      },
      "source": [
        "#### Setup\n",
        "\n",
        "Set the model to eval mode and move to desired device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "703119e4",
      "metadata": {
        "attributes": {
          "classes": [
            "python "
          ],
          "id": ""
        },
        "id": "703119e4"
      },
      "outputs": [],
      "source": [
        "# Set to GPU or CPU\n",
        "device = \"cuda\"\n",
        "model = model.eval()\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 15fnvK0KS9rQdoAB1yK2kw5WFVws_BlW5"
      ],
      "metadata": {
        "id": "19FWRf2ZohDW"
      },
      "id": "19FWRf2ZohDW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip behavior_videos.zip"
      ],
      "metadata": {
        "id": "nEGQ6k7ooocy"
      },
      "id": "nEGQ6k7ooocy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "video_annotation = \"behaivor_videos/amygdala control/CaMKII 1-19 L 2-19-21-Phase 3.csv\"\n",
        "df_anno = pd.read_csv(video_annotation)\n",
        "df_anno[df_anno.Behavior == 'Grooming']"
      ],
      "metadata": {
        "id": "HAH_Oyzfo30Y"
      },
      "id": "HAH_Oyzfo30Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "behaviors = df_anno.Behavior.unique().tolist()"
      ],
      "metadata": {
        "id": "-vnSALEDo9Iv"
      },
      "id": "-vnSALEDo9Iv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "behaviors = behaviors + ['FP Mouse Mounting Stimulus Mouse','FP Mouse Snigging Excretions','FP Mouse Tail Rattling','Others']"
      ],
      "metadata": {
        "id": "bahD7b61o-lt"
      },
      "id": "bahD7b61o-lt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "behaviors"
      ],
      "metadata": {
        "id": "DwGJ0dygpByp"
      },
      "id": "DwGJ0dygpByp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import csv\n",
        "import json\n",
        "import itertools\n",
        "from collections import defaultdict\n",
        "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "# Constants for file paths and split ratio\n",
        "BASE_FOLDER = \"behaivor_videos\"\n",
        "OUTPUT_VIDEO_FOLDER = \"behavior_video_clips\"\n",
        "TRAIN_JSONL_PATH = \"train_video_annotations.jsonl\"\n",
        "TEST_JSONL_PATH = \"test_video_annotations.jsonl\"\n",
        "TRAIN_SPLIT_RATIO = 0.97\n",
        "\n",
        "# Parameters for SlowFast data preparation\n",
        "SLOW_FPS_REDUCTION_FACTOR = 4  # Factor by which slow pathway FPS is reduced\n",
        "TARGET_FPS = 30 # Assuming original video FPS is around 30, adjust if needed\n",
        "\n",
        "# Ensure output folder exists\n",
        "os.makedirs(OUTPUT_VIDEO_FOLDER, exist_ok=True)\n",
        "\n",
        "def extract_video_segment(video_file_path, start_time, end_time, output_path):\n",
        "    \"\"\"Extracts a video segment from a file between start and end times.\"\"\"\n",
        "    ffmpeg_extract_subclip(video_file_path, start_time, end_time, targetname=output_path)\n",
        "\n",
        "def create_annotation_entry(behavior, video_segment_path_slow, video_segment_path_fast, prompt=\"<video> What is the behavior in the video?\"):\n",
        "    \"\"\"Creates a JSONL entry for a video segment, storing paths for both slow and fast pathways.\"\"\"\n",
        "    return {\n",
        "        \"query\": prompt,\n",
        "        \"response\": behavior,\n",
        "        \"videos\": [video_segment_path_slow, video_segment_path_fast]\n",
        "    }\n",
        "\n",
        "def parse_behavior_events(csv_path):\n",
        "    \"\"\"Parses start and stop events from a behavior CSV file.\"\"\"\n",
        "    start_events, stop_events = [], []\n",
        "    with open(csv_path, 'r') as csv_file:\n",
        "        reader = csv.DictReader(csv_file)\n",
        "        for row in reader:\n",
        "            time = float(row[\"Recording time\"])\n",
        "            behavior = row[\"Behavior\"]\n",
        "            event = row[\"Event\"]\n",
        "            if event == \"state start\":\n",
        "                start_events.append({\"time\": time, \"behavior\": behavior})\n",
        "            elif event == \"state stop\":\n",
        "                stop_events.append({\"time\": time, \"behavior\": behavior})\n",
        "    return start_events, stop_events\n",
        "\n",
        "def find_gaps(start_events, stop_events, video_duration, gap_duration):\n",
        "    \"\"\"Finds gaps between behavior events to sample as 'Others'.\"\"\"\n",
        "    gaps = []\n",
        "    last_end_time = 0\n",
        "\n",
        "    for start_event in start_events:\n",
        "        start_time = start_event[\"time\"]\n",
        "        if start_time - last_end_time >= gap_duration:\n",
        "            gaps.append((last_end_time, start_time))\n",
        "\n",
        "        matching_stop = next((s for s in stop_events\n",
        "                              if s[\"behavior\"] == start_event[\"behavior\"] and s[\"time\"] > start_time), None)\n",
        "        if matching_stop:\n",
        "            last_end_time = matching_stop[\"time\"]\n",
        "            stop_events.remove(matching_stop)\n",
        "\n",
        "    if video_duration - last_end_time >= gap_duration:\n",
        "        gaps.append((last_end_time, video_duration))\n",
        "\n",
        "    return gaps\n",
        "\n",
        "def sample_limited_segments_from_gaps(gaps, video_file_path, video_name, gap_duration, max_count, behavior_label=\"Others\"):\n",
        "    \"\"\"Samples segments from the gaps with a limit on the number of segments for 'Others' behavior, creating slow and fast versions.\"\"\"\n",
        "    entries = []\n",
        "    for start, end in gaps:\n",
        "        num_segments = int((end - start) // gap_duration)\n",
        "        for i in range(min(num_segments, max_count - len(entries))):\n",
        "            segment_start = start + i * gap_duration\n",
        "            segment_end = segment_start + gap_duration\n",
        "\n",
        "            # Create paths for slow and fast segments (both are initially the same temporal segment)\n",
        "            segment_path_slow = f\"{OUTPUT_VIDEO_FOLDER}/{video_name}_other_slow_{segment_start}-{segment_end}.mp4\"\n",
        "            segment_path_fast = f\"{OUTPUT_VIDEO_FOLDER}/{video_name}_other_fast_{segment_start}-{segment_end}.mp4\"\n",
        "\n",
        "            # Extract both segments\n",
        "            extract_video_segment(video_file_path, segment_start, segment_end, segment_path_slow)\n",
        "            extract_video_segment(video_file_path, segment_start, segment_end, segment_path_fast)\n",
        "\n",
        "            entries.append(create_annotation_entry(behavior_label, segment_path_slow, segment_path_fast))\n",
        "            if len(entries) >= max_count:\n",
        "                break\n",
        "        if len(entries) >= max_count:\n",
        "            break\n",
        "    return entries\n",
        "\n",
        "def process_video_file(csv_path, video_file_path, gap_duration=5):\n",
        "    \"\"\"Processes a video file by extracting labeled and limited 'Others' segments, creating slow and fast versions.\"\"\"\n",
        "    from moviepy.editor import VideoFileClip\n",
        "    start_events, stop_events = parse_behavior_events(csv_path)\n",
        "    video_name = os.path.splitext(os.path.basename(video_file_path))[0].replace(\" \", \"_\")\n",
        "    labeled_entries = []\n",
        "    behavior_counts = defaultdict(int)\n",
        "\n",
        "    for start_event in start_events:\n",
        "        start_time = start_event[\"time\"]\n",
        "        behavior = start_event[\"behavior\"]\n",
        "        matching_stop = next(\n",
        "            (stop for stop in stop_events if stop[\"behavior\"] == behavior and stop[\"time\"] > start_time), None)\n",
        "\n",
        "        if matching_stop:\n",
        "            end_time = matching_stop[\"time\"]\n",
        "            stop_events.remove(matching_stop)\n",
        "\n",
        "            # Create paths for slow and fast segments\n",
        "            segment_path_slow = f\"{OUTPUT_VIDEO_FOLDER}/{video_name}_{behavior.replace(' ', '_')}_slow_{start_time}-{end_time}.mp4\"\n",
        "            segment_path_fast = f\"{OUTPUT_VIDEO_FOLDER}/{video_name}_{behavior.replace(' ', '_')}_fast_{start_time}-{end_time}.mp4\"\n",
        "\n",
        "            # Extract both segments\n",
        "            extract_video_segment(video_file_path, start_time, end_time, segment_path_slow)\n",
        "            extract_video_segment(video_file_path, start_time, end_time, segment_path_fast)\n",
        "\n",
        "            labeled_entries.append(create_annotation_entry(behavior, segment_path_slow, segment_path_fast))\n",
        "            behavior_counts[behavior] += 1\n",
        "\n",
        "    max_behavior_count = max(behavior_counts.values(), default=0)\n",
        "\n",
        "    with VideoFileClip(video_file_path) as video:\n",
        "        video_duration = video.duration\n",
        "    gaps = find_gaps(start_events, stop_events, video_duration, gap_duration)\n",
        "    other_entries = sample_limited_segments_from_gaps(gaps, video_file_path, video_name, gap_duration, max_behavior_count)\n",
        "\n",
        "    return labeled_entries + other_entries\n",
        "\n",
        "def stratified_interleaved_split_and_save_annotations(entries, train_path, test_path, train_ratio=0.97):\n",
        "    \"\"\"Splits annotations into stratified, interleaved train and test sets by behavior and saves them to JSONL files.\"\"\"\n",
        "    behavior_groups = defaultdict(list)\n",
        "    for entry in entries:\n",
        "        behavior = entry['response']\n",
        "        behavior_groups[behavior].append(entry)\n",
        "\n",
        "    train_entries, test_entries = [], []\n",
        "    for behavior, group_entries in behavior_groups.items():\n",
        "        random.shuffle(group_entries)\n",
        "        split_index = int(len(group_entries) * train_ratio)\n",
        "        train_entries.append(group_entries[:split_index])\n",
        "        test_entries.append(group_entries[split_index:])\n",
        "\n",
        "    interleaved_train = list(itertools.chain.from_iterable(itertools.zip_longest(*train_entries)))\n",
        "    interleaved_test = list(itertools.chain.from_iterable(itertools.zip_longest(*test_entries)))\n",
        "\n",
        "    interleaved_train = [entry for entry in interleaved_train if entry is not None]\n",
        "    interleaved_test = [entry for entry in interleaved_test if entry is not None]\n",
        "\n",
        "    with open(train_path, 'w') as f:\n",
        "        for entry in interleaved_train:\n",
        "            f.write(json.dumps(entry) + \"\\n\")\n",
        "\n",
        "    with open(test_path, 'w') as f:\n",
        "        for entry in interleaved_test:\n",
        "            f.write(json.dumps(entry) + \"\\n\")\n",
        "\n",
        "def process_dataset():\n",
        "    \"\"\"Processes the entire dataset and creates stratified interleaved train/test JSONL files for SlowFast.\"\"\"\n",
        "    all_entries = []\n",
        "    for subdir in os.listdir(BASE_FOLDER):\n",
        "        subdir_path = os.path.join(BASE_FOLDER, subdir)\n",
        "        if os.path.isdir(subdir_path):\n",
        "            for file in os.listdir(subdir_path):\n",
        "                if file.endswith('.csv'):\n",
        "                    csv_path = os.path.join(subdir_path, file)\n",
        "                    video_file_path = csv_path.replace('.csv', '.mpg')\n",
        "                    if os.path.exists(video_file_path):\n",
        "                        entries = process_video_file(csv_path, video_file_path)\n",
        "                        all_entries.extend(entries)\n",
        "\n",
        "    stratified_interleaved_split_and_save_annotations(\n",
        "        all_entries, TRAIN_JSONL_PATH, TEST_JSONL_PATH, TRAIN_SPLIT_RATIO)\n",
        "    print(\"Conversion complete. Stratified interleaved training and testing datasets created for SlowFast.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    process_dataset()"
      ],
      "metadata": {
        "id": "tO3JwcGYq7Cf"
      },
      "id": "tO3JwcGYq7Cf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an id to label name mapping\n",
        "behaviors_id_to_classname = {}\n",
        "for i, v in enumerate(behaviors):\n",
        "    behaviors_id_to_classname[i] = v"
      ],
      "metadata": {
        "id": "c8oIM0e409Yi"
      },
      "id": "c8oIM0e409Yi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "behaviors_id_to_classname"
      ],
      "metadata": {
        "id": "cBKINsdf1XAo"
      },
      "id": "cBKINsdf1XAo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the number of classes in your fine-tuning dataset\n",
        "num_classes = len(behaviors)\n",
        "\n",
        "# Modify the final classification layer\n",
        "# The correct attribute name is likely 'projection' within the 'head' module\n",
        "model.blocks[-1].proj = nn.Linear(model.blocks[-1].proj.in_features, num_classes)"
      ],
      "metadata": {
        "id": "VfyHXTevpWk3"
      },
      "id": "VfyHXTevpWk3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "041a0753",
      "metadata": {
        "id": "041a0753"
      },
      "source": [
        "#### Define input transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "260eb786",
      "metadata": {
        "id": "260eb786"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.hub import load\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pytorchvideo.data.encoded_video import EncodedVideo\n",
        "from torchvision.transforms import Compose, Lambda\n",
        "from torchvision.transforms._transforms_video import (\n",
        "    CenterCropVideo,\n",
        "    NormalizeVideo,\n",
        ")\n",
        "from pytorchvideo.transforms import (\n",
        "    ApplyTransformToKey,\n",
        "    ShortSideScale,\n",
        "    UniformTemporalSubsample,\n",
        ")\n",
        "import os\n",
        "import json\n",
        "import torch.optim as optim\n",
        "\n",
        "class PackPathway(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Transform for converting video frames as a list of tensors.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, frames: torch.Tensor):\n",
        "        fast_pathway = frames\n",
        "        # Perform temporal sampling from the fast pathway.\n",
        "        slow_pathway = torch.index_select(\n",
        "            frames,\n",
        "            1,\n",
        "            torch.linspace(\n",
        "                0, frames.shape[1] - 1, frames.shape[1] // 4  # slowfast_alpha is usually 4\n",
        "            ).long(),\n",
        "        )\n",
        "        frame_list = [slow_pathway, fast_pathway]\n",
        "        return frame_list\n",
        "\n",
        "class SlowFastDataset(Dataset):\n",
        "    def __init__(self, annotations_file, transform=None, clip_duration=None, class_names=None):\n",
        "        self.annotations = [json.loads(line) for line in open(annotations_file, 'r')]\n",
        "        self.transform = transform\n",
        "        self.clip_duration = clip_duration\n",
        "        self.class_names = class_names  # Add class_names\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        annotation = self.annotations[idx]\n",
        "        video_path_slow = annotation['videos'][0]\n",
        "        video_path_fast = annotation['videos'][1]\n",
        "        behavior_label = annotation['response']\n",
        "\n",
        "        # Convert behavior label to numerical index\n",
        "        label = self.class_names.index(behavior_label) if self.class_names else behavior_label\n",
        "        label = torch.tensor(label) # Ensure label is a tensor\n",
        "\n",
        "        video_slow = EncodedVideo.from_path(video_path_slow)\n",
        "        video_fast = EncodedVideo.from_path(video_path_fast)\n",
        "\n",
        "        start_sec = 0\n",
        "        end_sec = self.clip_duration\n",
        "\n",
        "        clip_slow_data = video_slow.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
        "        clip_fast_data = video_fast.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
        "\n",
        "        if self.transform:\n",
        "            frames_slow = self.transform[\"video_slow\"](clip_slow_data[\"video\"])\n",
        "            frames_fast = self.transform[\"video_fast\"](clip_fast_data[\"video\"])\n",
        "            return [frames_slow, frames_fast], label\n",
        "        else:\n",
        "            return [clip_slow_data[\"video\"], clip_fast_data[\"video\"]], label\n",
        "\n",
        "# Define your transforms\n",
        "side_size = 256\n",
        "mean = [0.45, 0.45, 0.45]\n",
        "std = [0.225, 0.225, 0.225]\n",
        "crop_size = 256\n",
        "num_frames = 32\n",
        "sampling_rate = 2\n",
        "frames_per_second = 30\n",
        "clip_duration = (num_frames * sampling_rate) / frames_per_second\n",
        "\n",
        "transform_slow = Compose([\n",
        "    UniformTemporalSubsample(num_frames // 4),\n",
        "    Lambda(lambda x: x / 255.0),\n",
        "    NormalizeVideo(mean, std),\n",
        "    ShortSideScale(size=side_size),\n",
        "    CenterCropVideo(crop_size=(crop_size, crop_size)),\n",
        "])\n",
        "\n",
        "transform_fast = Compose([\n",
        "    UniformTemporalSubsample(num_frames),\n",
        "    Lambda(lambda x: x / 255.0),\n",
        "    NormalizeVideo(mean, std),\n",
        "    ShortSideScale(size=side_size),\n",
        "    CenterCropVideo(crop_size=(crop_size, crop_size)),\n",
        "])\n",
        "\n",
        "train_transform = {\"video_slow\": transform_slow, \"video_fast\": transform_fast}\n",
        "val_transform = {\"video_slow\": transform_slow, \"video_fast\": transform_fast}\n",
        "\n",
        "# Assuming you have a list of your class names\n",
        "class_names = behaviors\n",
        "\n",
        "# Instantiate your datasets\n",
        "train_dataset = SlowFastDataset(\n",
        "    annotations_file='train_video_annotations.jsonl',\n",
        "    transform=train_transform,\n",
        "    clip_duration=clip_duration,\n",
        "    class_names=class_names  # Pass class names to the dataset\n",
        ")\n",
        "\n",
        "val_dataset = SlowFastDataset(\n",
        "    annotations_file='test_video_annotations.jsonl',\n",
        "    transform=val_transform,\n",
        "    clip_duration=clip_duration,\n",
        "    class_names=class_names  # Pass class names here as well\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data loaders\n",
        "batch_size = 1\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "X7i2smHIulUa"
      },
      "id": "X7i2smHIulUa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define loss function and optimizer\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs = [inp.to(device) for inp in inputs]\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss / 10:.4f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs = [inp.to(device) for inp in inputs]\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Accuracy: {100 * correct / total:.2f}%')\n",
        "\n"
      ],
      "metadata": {
        "id": "qmQSsNcXtdUn"
      },
      "id": "qmQSsNcXtdUn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Finished Training')\n",
        "torch.save(model.state_dict(), 'fine_tuned_slowfast_model.pth')"
      ],
      "metadata": {
        "id": "54iXvF1Q2M3w"
      },
      "id": "54iXvF1Q2M3w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform =  ApplyTransformToKey(\n",
        "    key=\"video\",\n",
        "    transform=Compose(\n",
        "        [\n",
        "            UniformTemporalSubsample(num_frames),\n",
        "            Lambda(lambda x: x/255.0),\n",
        "            NormalizeVideo(mean, std),\n",
        "            ShortSideScale(\n",
        "                size=side_size\n",
        "            ),\n",
        "            CenterCropVideo(crop_size),\n",
        "            PackPathway()\n",
        "        ]\n",
        "    ),\n",
        ")\n",
        "\n",
        "# The duration of the input clip is also specific to the model.\n",
        "clip_duration = (num_frames * sampling_rate)/frames_per_second"
      ],
      "metadata": {
        "id": "DP8-sHZKzNJ9"
      },
      "id": "DP8-sHZKzNJ9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "16b1a4b6",
      "metadata": {
        "id": "16b1a4b6"
      },
      "source": [
        "#### Run Inference\n",
        "\n",
        "Download an example video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8666b5f",
      "metadata": {
        "id": "b8666b5f"
      },
      "outputs": [],
      "source": [
        "video_path = '/content/behaivor_videos/amygdala control/CaMKII 1-19 L 2-19-21-Phase 3.mpg'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53d7d0a6",
      "metadata": {
        "id": "53d7d0a6"
      },
      "source": [
        "Load the video and transform it to the input format required by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50bc7f72",
      "metadata": {
        "id": "50bc7f72"
      },
      "outputs": [],
      "source": [
        "# Select the duration of the clip to load by specifying the start and end duration\n",
        "# The start_sec should correspond to where the action occurs in the video\n",
        "start_sec = 0\n",
        "end_sec = start_sec + clip_duration\n",
        "\n",
        "# Initialize an EncodedVideo helper class and load the video\n",
        "video = EncodedVideo.from_path(video_path)\n",
        "\n",
        "# Load the desired clip\n",
        "video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
        "\n",
        "# Apply a transform to normalize the video input\n",
        "video_data = transform(video_data)\n",
        "\n",
        "# Move the inputs to the desired device\n",
        "inputs = video_data[\"video\"]\n",
        "inputs = [i.to(device)[None, ...] for i in inputs]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f071647f",
      "metadata": {
        "id": "f071647f"
      },
      "source": [
        "#### Get Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44666862",
      "metadata": {
        "id": "44666862"
      },
      "outputs": [],
      "source": [
        "# Pass the input clip through the model\n",
        "preds = model(inputs)\n",
        "\n",
        "# Get the predicted classes\n",
        "post_act = torch.nn.Softmax(dim=1)\n",
        "preds = post_act(preds)\n",
        "pred_classes = preds.topk(k=5).indices[0]\n",
        "\n",
        "# Map the predicted classes to the label names\n",
        "pred_class_names = [behaviors_id_to_classname[int(i)] for i in pred_classes]\n",
        "print(\"Top 5 predicted labels: %s\" % \", \".join(pred_class_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a991646",
      "metadata": {
        "id": "4a991646"
      },
      "source": [
        "### Model Description\n",
        "SlowFast model architectures are based on [1] with pretrained weights using the 8x8 setting\n",
        "on the Kinetics dataset.\n",
        "\n",
        "| arch | depth | frame length x sample rate | top 1 | top 5 | Flops (G) | Params (M) |\n",
        "| --------------- | ----------- | ----------- | ----------- | ----------- | ----------- |  ----------- | ----------- |\n",
        "| SlowFast | R50   | 8x8                        | 76.94 | 92.69 | 65.71     | 34.57      |\n",
        "| SlowFast | R101  | 8x8                        | 77.90 | 93.27 | 127.20    | 62.83      |\n",
        "\n",
        "\n",
        "### References\n",
        "[1] Christoph Feichtenhofer et al, \"SlowFast Networks for Video Recognition\"\n",
        "https://arxiv.org/pdf/1812.03982.pdf"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}