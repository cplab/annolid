{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KruxdFHpCWGk"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/healthonrails/annolid/blob/main/docs/tutorials/YOLO_SAHI_inference_for_ultralytics.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW_Fs8O0CWGl"
      },
      "source": [
        "## 0. Preperation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwGC2CsiCWGm"
      },
      "source": [
        "- Install latest version of SAHI and ultralytics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lauam-B1CWGm"
      },
      "outputs": [],
      "source": [
        "!pip install -U torch sahi ultralytics\n",
        "!pip install ipywidgets\n",
        "!pip install supervision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFmY-qvUCWGo"
      },
      "source": [
        "## 1. Sliced Inference with an Ultralytics Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bU0MBuZxCWGo"
      },
      "source": [
        "- Instantiate a detection model by defining model weight path and other parameters:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "from sahi import AutoDetectionModel\n",
        "from sahi.predict import get_sliced_prediction\n",
        "from google.colab.patches import cv2_imshow\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Initialize the detection model (using Ultralytics YOLO weights)\n",
        "detection_model = AutoDetectionModel.from_pretrained(\n",
        "    model_type='ultralytics',\n",
        "    model_path=\"yolo11n.pt\",  # Use the appropriate model weight file\n",
        "    confidence_threshold=0.35,\n",
        "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        ")\n",
        "\n",
        "def process_video(input_video_path, output_video_path,\n",
        "                  slice_height=256, slice_width=256,\n",
        "                  overlap_height_ratio=0.2, overlap_width_ratio=0.2,\n",
        "                  export_dir=\"/content/sahi_video_frames\"):\n",
        "    \"\"\"\n",
        "    Processes a video frame-by-frame by running sliced prediction on each frame.\n",
        "    Annotated frames are generated via SAHI's export_visuals method (which saves a PNG file).\n",
        "    The PNG is then read back as a NumPy array for writing to the output video.\n",
        "    Every 30 frames the annotated frame is displayed inline (suitable for Colab).\n",
        "\n",
        "    Args:\n",
        "        input_video_path (str): Path to the input video file.\n",
        "        output_video_path (str): Path to save the annotated output video.\n",
        "        slice_height (int): Height of each slice used during inference.\n",
        "        slice_width (int): Width of each slice used during inference.\n",
        "        overlap_height_ratio (float): Overlap ratio for slice height.\n",
        "        overlap_width_ratio (float): Overlap ratio for slice width.\n",
        "        export_dir (str): Directory to save temporary annotated frames.\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(input_video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Could not open video file.\")\n",
        "        return\n",
        "\n",
        "    # Create the export directory if it doesn't exist.\n",
        "    os.makedirs(export_dir, exist_ok=True)\n",
        "\n",
        "    # Retrieve video properties.\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "    # Set up the video writer for the output video.\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "    frame_count = 0\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Run sliced prediction on the current frame.\n",
        "        # get_sliced_prediction returns a PredictionResult instance.\n",
        "        result = get_sliced_prediction(\n",
        "            frame,\n",
        "            detection_model,\n",
        "            slice_height=slice_height,\n",
        "            slice_width=slice_width,\n",
        "            overlap_height_ratio=overlap_height_ratio,\n",
        "            overlap_width_ratio=overlap_width_ratio,\n",
        "        )\n",
        "\n",
        "        # Use SAHI's export_visuals to annotate the frame.\n",
        "        # A unique file name is generated for each frame.\n",
        "        file_name = f\"frame_{frame_count}\"\n",
        "        result.export_visuals(\n",
        "            export_dir=export_dir,\n",
        "            file_name=file_name,\n",
        "            text_size=1,\n",
        "            rect_th=2,\n",
        "            hide_conf=False\n",
        "        )\n",
        "\n",
        "        # Read the annotated frame from disk.\n",
        "        annotated_frame_path = os.path.join(export_dir, f\"{file_name}.png\")\n",
        "        annotated_frame = cv2.imread(annotated_frame_path)\n",
        "\n",
        "        # Write the annotated frame to the output video.\n",
        "        out.write(annotated_frame)\n",
        "\n",
        "        # Every 30 frames, display the annotated frame inline.\n",
        "        if frame_count % 30 == 0:\n",
        "            cv2_imshow(annotated_frame)\n",
        "            clear_output(wait=True)\n",
        "            print(f\"Processed {frame_count} frames...\")\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(\"Video processing complete.\")"
      ],
      "metadata": {
        "id": "_zMPhYcaGzDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "input_video = \"/content/video-4.mp4\"\n",
        "output_video = \"/video-4_tracked.mp4\"\n",
        "process_video(input_video, output_video)"
      ],
      "metadata": {
        "id": "HzIBjAEiHF4W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}